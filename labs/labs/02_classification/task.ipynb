{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A base de dados que utilizaremos é conhecida como MNIST (\"Modified National Institute of Standards and Technology\"), e deriva de uma base maior que foi construida pela NIST nos Estados Unidos (o equivalente da nossa ABNT). Esta base de dados é considerada o verdadeiro \"Hello, world!\" de métodos de classificação. Em http://yann.lecun.com/exdb/mnist/ temos uma descrição mais detalhada desta base de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade**: \n",
    "    \n",
    "Leia a página de descrição do MNIST supracitada, e responda:\n",
    "\n",
    "- Quantas imagens de treinamento e quantas imagens de teste existem na MNIST?\n",
    "\n",
    "- Qual o tamanho de cada imagem no MNIST?\n",
    "\n",
    "- Os criadores da MNIST tiveram um cuidado especial ao construir os conjuntos de treinamento e teste, em relação às pessoas que escreveram os dígitos. Que cuidado foi esse, e por que foi adotado?\n",
    "\n",
    "- As imagens foram construidas escaneando digitos manuscritos, que foram escritos por dois grupos de pessoas: alunos de colegial (SD-1) e funcionários da NIST (SD-3). Originalmente a NIST designou SD-3 como o conjunto de teste, e SD-1 como o conjunto de treinamento. Os criadores da MNIST criticaram essa decisão e resolveram misturar os conjuntos. Por que? Como esta situação difere daquela da pergunta anterior?\n",
    "\n",
    "- A página lista vários artigos que exploraram métodos de classificação no MNIST, com seus respectivos desempenhos. Qual o método com o pior desempenho (e qual foi esse desempenho)? Qual o método com o melhor desempenho, e de quanto foi?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O scikit-learn já tem ferramentas para baixar e disponibilizar alguns dos datasets mais comuns da comunidade de machine learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "\n",
    "# A função fetch_openml() returns targets as strings, precisamos converter para\n",
    "# valores numéricos.\n",
    "mnist.target = mnist.target.astype(np.int8)\n",
    "\n",
    "mnist[\"data\"], mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No campo `data` temos as várias imagens de dígitos manuscritos. Cada item é uma lista de $28^2 = 784$ valores.\n",
    "\n",
    "No campo `target` temos o rótulo de cada uma dessas imagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist['data'].to_numpy(), mnist['target'].to_numpy()\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver um desses dígitos manuscritos para checar se a leitura de dados funcionou:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_digit = X[0]\n",
    "some_digit_label = y[0]\n",
    "\n",
    "print(f'label: {some_digit_label}')\n",
    "\n",
    "some_digit_image = some_digit.reshape(28, 28)  # Por que? Explique!\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(\n",
    "    some_digit_image,\n",
    "    cmap=matplotlib.cm.binary,\n",
    "    interpolation='nearest',\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que deu certo: pela imagem trata-se de um dígito $5$ manuscrito, e de fato o rótulo confirma essa observação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separando treinamento e teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conforme visto na descrição do dataset MNIST, a separação entre conjunto de treinamento e teste já está feita. Neste caso, não devemos fazer a separação dos dados conforme visto na aula passada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(mnist['target'][:60000], return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f'{u}: {c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os primeiros 60000 exemplos são o conjunto de treinamento, e estão organizados por dígito. Os últimos 10000 exemplos são o conjunto de teste, e também estão organizados por dígito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:60000]\n",
    "y_train = y[:60000]\n",
    "\n",
    "X_test = X[60000:]\n",
    "y_test = y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver mais alguns digitos desta base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digits(instances, images_per_row=10, **options):\n",
    "    \"\"\"Plota uma lista de imagens do MNIST em um formato de matriz de imagens.\n",
    "    \n",
    "    Args:\n",
    "        instances: Lista de linhas da matriz de amostras do MNIST.\n",
    "        images_per_row: Número de imagens por linha.\n",
    "        options: Opções passadas ao comando plt.imshow() para desenho.\n",
    "    \"\"\"\n",
    "    # Tamanho das imagens no MNIST: 28 x 28.\n",
    "    size = 28\n",
    "\n",
    "    # Monta uma lista de imagens usando list comprehension e np.reshape().\n",
    "    # Cada item da lista resultante é uma imagem 28 x 28.\n",
    "    images = [instance.reshape(size, size) for instance in instances]\n",
    "\n",
    "    # Caso o número de imagens por linha seja muito grande,\n",
    "    # limite no número de imagens disponível.\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "\n",
    "    # Dado o número de imagens por linha, calcule quantas linhas\n",
    "    # são necessárias.\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "\n",
    "    # Truque: cria uma imagem em branco de tamanho suficiente para preencher\n",
    "    # o espaço em branco no final da ultima linha, e coloca essa imagem\n",
    "    # em branco na lista de imagens.\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "\n",
    "    # Cria uma imagem unificada por linha.\n",
    "    row_images = []\n",
    "    for row in range(n_rows):\n",
    "        # Junta imagens da linha em uma imagem unificada, e coloca na\n",
    "        # lista de imagens de linha.\n",
    "        rimages = images[row * images_per_row:(row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "\n",
    "    # Junta todas as imagens de linha em uma unica imagem final.\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "\n",
    "    # Mostra a imagem final.\n",
    "    plt.imshow(image, cmap=matplotlib.cm.binary, **options)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "\n",
    "example_labels = y[:100]\n",
    "example_images = X[:100]\n",
    "\n",
    "print(example_labels)\n",
    "\n",
    "plt.figure(figsize=(9, 9))\n",
    "plot_digits(example_images, images_per_row=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação binária"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos começar com um problema mais simples: classificar os dígitos da base em 'cincos' e 'não-cincos'. Este é um problema de classificação binária. Por mera convenção, chamaremos de 'amostras positivas', ou simplemente 'positivos' os digitos $5$ e de 'negativos' os demais dígitos.\n",
    "\n",
    "Vamos adaptar os conjuntos de treinamento e teste ao nosso cenário:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_5 = (y_train == 5)\n",
    "y_test_5 = (y_test == 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificando se funcionou:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "for original, binarized in zip(y_train[:n], y_train_5[:n]):\n",
    "    print(f'{original} -> {binarized}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora treinar um classificador sobre todo o conjunto de treinamento, como fizemos na aula sobre regressão. Vamos usar um classificador chamado de *Stochastic Gradient Descent*, que é uma generalização de alguns tipos diferentes de classificadores mais tradicionais. O scikit-learn tem uma classe que implementa este classificador: `SGDClassifier`. Com os parâmetros default desta classe, o classificador SGD é equivalente a um classificador do tipo \"máquina de vetores de suporte linear\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Existe aleatoriedade dentro do SGDClassifier, por isso o argumento\n",
    "# random_state=RANDOM_SEED.\n",
    "sgd_clf = SGDClassifier(\n",
    "    max_iter=500,\n",
    "    tol=1e-3,\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "sgd_clf.fit(X_train, y_train_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos testar o classificador naquele dígito $5$ que a gente tinha visualizado no começo do notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe que a amostra \"some_digit\" é apenas uma lista de valores de pixel.\n",
    "# O comando predict requer uma matriz ou uma lista de listas, onde cada linha é\n",
    "# uma amostra a ser classificada (mesmo que seja uma amostra só!). Portanto,\n",
    "# temos que colocar \"some_digit\" numa lista.\n",
    "sgd_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfeito, ele acertou! Mas isso foi apenas um exemplo, vamos agora estudar o desempenho do classificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas de desempenho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando validação cruzada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim como na aula anterior, podemos usar a estratégia da validação cruzada para tentar inferir o desempenho do nosso classificador no mundo real:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "t1 = time.process_time()\n",
    "res = cross_val_score(\n",
    "    sgd_clf,\n",
    "    X_train,\n",
    "    y_train_5,\n",
    "    cv=3,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "t2 = time.process_time()\n",
    "\n",
    "print('Elapsed time: {}'.format(t2 - t1))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A medida usada é o *accuracy* (acurácia), que é a porcentagem de acertos de previsão. Obtivemos 96%! Parece excelente, mas será mesmo? Compare com o \"classificador\" a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "\n",
    "class Never5Classifier(BaseEstimator):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Nada a ser feito no treinamento.\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Recebe len(X) amostras, chuta \"False\" como resposta para todas!\n",
    "        return [False] * len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "never_5_clf = Never5Classifier()\n",
    "cross_val_score(\n",
    "    never_5_clf,\n",
    "    X_train,\n",
    "    y_train_5,\n",
    "    cv=3,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pergunta:** Parece que atingir 90% não é nada difícil neste problema... na verdade, é o esperado! Explique porque."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz de confusão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma forma muito interessante de se avaliar o desempenho de um classificador é obter a matriz de confusão (*confusion matrix*) do classificador. Nesta matriz cada linha representa a categoria *verdadeira* de um objeto, e cada coluna representa a categoria *predita* de um objeto. Uma posição $(r,c)$ da matriz de confusão representa, portanto, o número de objetos que pertencem verdadeiramente à categoria $r$, mas que foram classificados como pertencentes à categoria $c$ por nosso classificador. \n",
    "\n",
    "<center>\n",
    "<img src=\"confusao.png\" alt=\"Matriz de confusão\">\n",
    "</center>\n",
    "\n",
    "As células da diagonal, em azul, mostram as posições onde a classe verdadeira e a classe predita coincidem, esses são os acertos. As células fora da diagonal, em vermelho, são os erros.\n",
    "\n",
    "Podemos calcular a matriz de confusão resultante do treinamento sobre o conjunto (de treinamento) completo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sgd_clf.fit(X_train, y_train_5)\n",
    "y_train_pred = sgd_clf.predict(X_train)\n",
    "mat = confusion_matrix(y_train_5, y_train_pred)\n",
    "mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porém o desempenho exibido por este processo é muito otimista, e não representa uma estimativa realista dos erros deste classificador no mundo real.\n",
    "\n",
    "**Pergunta:** Em caso de overfitting total, como ficaria a matriz de confusão?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma idéia melhor é aplicar o conceito de validação cruzada para realizar a predição de cada amostra. Funciona assim:\n",
    "\n",
    "- Particionamos os dados em N partições.\n",
    "\n",
    "- Para cada partição:\n",
    "\n",
    "    - Treinamos o classificador sobre os dados das outras partições\n",
    "    \n",
    "    - Usamos o classificador para prever as classes das amostras desta partição\n",
    "\n",
    "Por exemplo: suponha que temos 3 partições. As categorias preditas dos objetos da primeira partição são obtidas da seguinte forma:\n",
    "\n",
    "- Treinamos o classificador usando os dados das partições 2 e 3\n",
    "\n",
    "- Aplicamos o classificador para os objetos da partição 1. Guardamos estes resultados\n",
    "\n",
    "Fazemos o mesmo para os objetos das partições 2 e 3. Desta forma, cada objeto foi predito de modo \"honesto\", ou seja, usando um classificador que não continha o próprio objeto como dado de treinamento!\n",
    "\n",
    "O scikit-learn já tem uma função para fazer exatamente isso: `cross_val_predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos observar uma matriz de confusão mais realista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(y_train_5, y_train_pred)\n",
    "mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não mudou muito, mas tudo bem: esses números são mais confiáveis. Significa que nosso modelo não é muito inclinado a ter overfitting nesse problema em particular!\n",
    "\n",
    "Nesta matriz de confusão a primeira linha indica dígitos \"não-cinco\", e a segunda linha indica os dígitos \"cinco\". Em problemas de classificação binária usamos a terminologia \"negativos\" (os \"não-cinco\") e \"positivos\" (os \"cinco\"), e dizemos também que nosso problema é \"detectar\" os dígitos \"cinco\".\n",
    "\n",
    "- **TP**: Os valores **verdadeiramente positivos** e que foram **preditos como positivos** são os **true positives** (verdadeiros positivos).\n",
    "\n",
    "- **FN**: Os valores **verdadeiramente positivos** e que foram **preditos como negativos** são os **false negatives** (falsos negativos, pois foram errôneamente classificados como negativos).\n",
    "\n",
    "- **TN**: Os valores **verdadeiramente negativos** e que foram **preditos como negativos** são os **true negatives** (verdadeiros negativos).\n",
    "\n",
    "- **FP**: os valores **verdadeiramente negativos** e que foram **preditos como positivos** são os **false positives** (falsos positivos, pois foram errôneamente classificados como positivos).\n",
    "\n",
    "![Precision-recall](precision_recall.png \"Precision and recall\")\n",
    "\n",
    "**Perguntas:**\n",
    "\n",
    "- Dê um exemplo real de falso positivo.\n",
    "\n",
    "- Dê um exemplo real de falso negativo.\n",
    "\n",
    "- Nesta matriz exemplo, qual é a acurácia?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision e recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A medida de acurácia não permite distinguir entre os tipos de erro. Duas medidas mais comuns que são empregadas em machine learning são a **precision** (precisão) e **recall** (revocação), definidas como:\n",
    "\n",
    "- Precision: Dentre os elementos classificados como positivos, quantos realmente são positivos?\n",
    "\n",
    "$$\\text{precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "- Recall: Dentre os elementos verdadeiramente positivos, quantos foram detectados como positivos?\n",
    "\n",
    "$$\\text{recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "**Perguntas:** \n",
    "\n",
    "- É sempre possível construir um classificador com recall 100%. Como? E o que acontece com o precision?\n",
    "\n",
    "- Qual o precision e o recall do Never5Classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para calcular o precision e o recall no scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print(precision_score(y_train_5, y_train_pred))\n",
    "print(recall_score(y_train_5, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:** Confirme se o scikit-learn acertou baseado na matriz de confusão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A métrica $F_1$ serve para combinar o precision e o recall em uma métrica única que valoriza o equilibrio entre estas duas medidas. É definida como a média harmônica do precision e do recall:\n",
    "\n",
    "\n",
    "$$F_1 = \\frac{2}{\\frac{1}{\\text{precision}} + \\frac{1}{\\text{recall}}}$$\n",
    "\n",
    "No scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(f1_score(y_train_5, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O valor de $F_1$ tende a favorecer precision e recall balanceados. Isto não é necessariamente bom, existem situações em que você quer favorecer um ou outro.\n",
    "\n",
    "**Perguntas:**\n",
    "\n",
    "- Dê um exemplo de situação onde precision é melhor que recall.\n",
    "\n",
    "- Dê um exemplo de situação onde recall é melhor que precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision/recall tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seria muito bom se tivéssemos um classificador com precision 100% e recall 100%, seria um classificador perfeito!\n",
    "\n",
    "Infelizmente o mundo real não é assim: quanto maior o precision menor o recall, e vice versa. Para entender isso melhor temos que conhecer um pouco mais a fundo como nosso classificador (SGD) funciona.\n",
    "\n",
    "Dentro do SGDClassifier, o primeiro passo da predição é calcular um valor para a amostra sobre a qual estamos fazendo a predição. Veremos em aulas subsequentes como isso funciona. Quanto maior o valor, mais provável é que a amostra seja positiva. Esta função que se aplica inicialmente chama-se \"função de decisão\" (decision function). \n",
    "\n",
    "Em seguida, usamos um parâmetro do classificador chamado de valor de limiar (threshold). Se o valor da função de decisão estiver acima do threshold, a amostra é classificada como positiva. Caso contrário, será classificada como negativa. \n",
    "\n",
    "<center>\n",
    "<img src=\"decision_function.png\" alt=\"função de decisão\" style=\"width: 800px;\"/>\n",
    "</center>\n",
    "\n",
    "**Perguntas:**\n",
    "\n",
    "- O que acontece se o threshold for muito, muito baixo? Como ficam os valores de precision e recall?\n",
    "\n",
    "- O que acontece se o threshold for muito, muito alto? Como ficam os valores de precision e recall?\n",
    "\n",
    "- Prove que se o precision e o recall são $100\\%$ temos um classificador que não erra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar os valores da função de decisão calculados para nossas amostras de treinamento usando o scikit-learn: basta adicionar um parâmetro extra à chamada de `cross_val_predict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = cross_val_predict(\n",
    "    sgd_clf,\n",
    "    X_train,\n",
    "    y_train_5,\n",
    "    cv=3,\n",
    "    method=\"decision_function\",\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora plotar os valores de precision e recall juntos em uma curva única:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "\n",
    "plt.xlabel(\"Threshold\", fontsize=16)\n",
    "plt.legend(loc=\"upper left\", fontsize=16)\n",
    "plt.xlim([-40000, 40000])\n",
    "plt.ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que essa curva significa? Significa que podemos ter qualquer valor de precision que quisermos, mas isso mexe no recall, e vice versa! \n",
    "\n",
    "Por exemplo: suponha que queremos um precision de 90% - queremos que nosso classificador esteja muito seguro de que achou um dígito 5. Como já temos os valores da função de decisão (`y_scores`), basta aplicar um threshold alto para ter um classificador de alta precisão! Olhando na curva acima, vemos que um threshold de aproximadamente 8000 deve servir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_90 = (y_scores > 8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos medir o precision e o recall deste classificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision: {}'.format(precision_score(y_train_5, y_train_pred_90)))\n",
    "print('Recall: {}'.format(recall_score(y_train_5, y_train_pred_90)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aumentamos o precision para aproximadamente 90%, mas o recall caiu.\n",
    "\n",
    "Se estivermos interessados em observar apenas o compromisso entre precision e recall, podemos diagramar um contra o outro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot(recalls, precisions, \"b-\", linewidth=2)\n",
    "\n",
    "plt.xlabel(\"Recall\", fontsize=16)\n",
    "plt.ylabel(\"Precision\", fontsize=16)\n",
    "plt.axis([0, 1, 0, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que após recall de 80% o precision cai muito rápido.\n",
    "\n",
    "**Pergunta:** Alguem chega para você e diz \"Meu classificador é o melhor! Tem precision de 99%!\". O que você pergunta em seguida?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**\n",
    "\n",
    "- E qual o recall? E qual a proporção de cada classe no conjunto de treinamento e de teste?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensibilidade, especificidade e curva ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra ferramenta útil para descrever o desempenho de um classificador binário é a curva de Característica de Operação do Receptor, mais conhecida pelo seu nome em inglês: *Receiver Operating Characterístic (ROC) curve*. É uma curva similar á curva precision-recall, mas usa a razão de falsos positivos (*False Positive Rate - FPR*) no eixo das abscissas, e a razão de positivos verdadeiros (*True Positive Rate - TPR*) no eixo das ordenadas. \n",
    "\n",
    "*True Positive Rate* é o mesmo que recall: a fração dos verdadeiros positivos que foram identificados como positivos pelo classificador. Outro nome para esta quantidade é sensibilidade (*sensitivity*). \n",
    "\n",
    "$$\\text{TPR} = \\text{recall} = \\text{sensitivity} = \\frac{\\text{TP}}{\\text{P}} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$$\n",
    "\n",
    "Em termos simples, *sensitivity* é \"de todos os positivos, quantos eu detectei?\"\n",
    "\n",
    "O termo sensibilidade é muito usado na Medicina, em vista da importância da sensitividade no diagnóstico médico.\n",
    "\n",
    "**Pergunta:** No contexto de um programa de *screening* para detecção precoce do câncer de mama usando mamografias, o que significa alta sensitividade?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*False Positive Rate* é a fração dos negativos verdadeiros que foram identificados como positivos pelo classificador. \n",
    "\n",
    "$$\\text{FPR} = \\frac{FP}{N} = \\frac{FP}{TN + FP}$$\n",
    "\n",
    "Para entender o FPR, vamos entender outra quantidade que é o *True Negative Rate* (TNR), também conhecida como especificidade (*specificity*). A especificidade é a fração de negativos verdadeiros que foram identificados como negativos pelo classificador.\n",
    "\n",
    "$$\\text{TPR} = \\text{specificity} = \\frac{TN}{N} = \\frac{TN}{TN + FP}$$\n",
    "\n",
    "Em termos simples, *specificity* é \"de todos os negativos, quantos eu corretamente percebi como negativos?\"\n",
    "\n",
    "Combinando as expressões de FPR e TPR, vemos que:\n",
    "\n",
    "$$\\text{FPR} = 1 - \\text{specificity}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perguntas:** \n",
    "\n",
    "- Na questão anterior vimos que um programa de *screening* para câncer de mama deve ter alta sensitividade, para não ignorar mulheres (apesar de que homem tem câncer de mama também!) que estejam desenvolvendo a doença. Porém, com grande sensitividade vem baixa especificidade! O que acontece se for recomendado mastectomia em todos os casos detectados pelo *screening*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalmente, após uma detecção via *screening* a mulher é encaminhada para exames posteriores, incluindo biópsia. Em termos de sensibilidade e especificidade, o que esperamos de uma biópsia?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A curva ROC representa todos os pares (FPR, TPR) de um classificador binário que trabalhe com função de decisão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando escolhemos um valor específico de threshold para nosso classificador fixamos o ponto de trabalho deste na curva ROC. \n",
    "\n",
    "**Pergunta:** Um classificador perfeito opera em qual ponto do espaço $(\\text{FPR}, \\text{TPR})$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que acontece com um classificador aleatório (decide ao acaso o valor da função de decisão)? Este classificador terá uma curva ROC como a linha tracejada acima. Qualquer classificador melhor que aleatório terá uma curva ROC acima da linha tracejada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos trocar de classificador e ver o que acontece:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# O \"score\" vai ser a probabilidade de que a amostra seja da classe positiva.\n",
    "y_probas_forest = cross_val_predict(\n",
    "    forest_clf,\n",
    "    X_train,\n",
    "    y_train_5,\n",
    "    cv=3,\n",
    "    method=\"predict_proba\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Gambiarra para desviar do bug #9589 introduzido no Scikit-Learn 0.19.0:\n",
    "y_scores_forest = y_probas_forest[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_forest, tpr_forest, thresholds_forest = roc_curve(\n",
    "    y_train_5,\n",
    "    y_scores_forest,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, \"b:\", linewidth=2, label=\"SGD\")\n",
    "plt.plot(fpr_forest, tpr_forest, linewidth=2, label=\"Random Forest\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.legend(loc=\"lower right\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que o classificador RandomForest apresenta TPR mais alto para um mesmo FPR do que o SGD. Com isso, parece que o RandomForest tem melhor desempenho que o SGD.\n",
    "\n",
    "Para sumarizar o desempenho de um classificador binário em apenas um número, usamos a área sob a curva ROC (ROC AUC - Area Under the Curve - score). \n",
    "\n",
    "**Perguntas:** \n",
    "\n",
    "- Qual o valor do ROC AUC score para o classificador aleatório?\n",
    "\n",
    "- Qual o valor do ROC AUC score para um classificador perfeito?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pergunta:**\n",
    "\n",
    "O que você prefere ter: um classificador com $AUC = 0.75$ ou com $AUC = 0.1$? Justifique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos então comparar os escores ROC AUC dos dois classificadores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print('SGD: {:4f}'.format(roc_auc_score(y_train_5, y_scores)))\n",
    "print('RandomForest: {:4f}'.format(roc_auc_score(y_train_5, y_scores_forest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest é um classificador melhor do que SGD neste problema. Veja também em termos da curva precision-recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions_forest, recalls_forest, thresholds_forest = precision_recall_curve(\n",
    "    y_train_5, y_scores_forest)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(precisions, recalls, label='SGD')\n",
    "plt.plot(precisions_forest, recalls_forest, label='Random Forest')\n",
    "plt.legend(loc='lower left')\n",
    "plt.xlabel('Precision')\n",
    "plt.ylabel('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De fato, o classificador RandomForest é superior ao classificador SGD em qualquer valor de precisão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação multiclasse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um classificador cinco-ou-não-cinco pode até ser útil, mas o problema real que queremos resolver é descobrir qual o dígito a partir de sua imagem. **Classificadores multiclasse** servem para este propósito.\n",
    "\n",
    "Alguns algoritmos (como Random Forest) são intrinsecamente capazes de fazer classificação multi-classes. Outros são unicamente classificadores binários, como o SGD. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf.fit(X_train, y_train)  # Aqui estamos usando y_train, não y_train_5!\n",
    "forest_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf.predict_proba([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O algoritmo RandomForest estima uma probabilidade de $90\\%$ de que o dígito em questão seja um $5$, $1\\%$ de que seja um $2$, $8\\%$ de que seja um $3$ e $1\\%$ de que seja um $9$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-versus-One e One-versus-All"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma forma de fazer um classificador multi-classe à partir de um classificador exclusivamente binário é construir um classificador binário para cada classe, e comparar os scores resultantes de cada um. Como cada classificador é do tipo um-versus-outros, esta abordagem é conhecida como estratégia *one-versus-all* (OvA).\n",
    "\n",
    "Outra possibilidade é treinar um conjunto de classificadores binários comparando classe-versus-classe. Por exemplo: no caso dos dígitos, teríamos o classificador zero-versus-um, zero-versus-dois, etc, até o classificador oito-versus-nove, para um total de 45 classificadores. Em geral, para um problema de $N$ classes temos $N \\cdot (N - 1) / 2$ classificadores. Temos muito mais classificadores, mas cada um deles é treinado para resolver um problema muito mais específico (e.g. distinguir 5 de 7, ao invés de ter que distinguir 5 do resto). Ademais, o conjunto de treinamento de cada um destes classificadores especializados é muito menor.\n",
    "\n",
    "Esta estratégia é conhecida como *one-versus-one* (OvO).\n",
    "\n",
    "Quando usar qual deles (OvO versus OvA)? Alguns classificadores escalam mal com o número de amostras: nestes casos OvO é preferível. Em outros casos a simplicidade do OvA é melhor.\n",
    "\n",
    "Scikit-learn usa automaticamente OvA para seus classificadores binários que não possam ser multiclasse automaticamente - exceto para Support Vector Machines, para os quais OvO é preferível por questão de escalabilidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf = SGDClassifier(\n",
    "    max_iter=500,\n",
    "    tol=1e-3,\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1,\n",
    ")  # Existe aleatoriedade dentro do SGDClassifier.\n",
    "\n",
    "sgd_clf.fit(X_train, y_train)  # Aqui estamos usando y_train, não y_train_5!\n",
    "sgd_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para confirmar esse resultado, observe os valores da função de decisão para cada uma das classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_digit_scores = sgd_clf.decision_function([some_digit])\n",
    "print(sgd_clf.classes_)\n",
    "print(some_digit_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que o maior score corresponde à classe $5$ realmente.\n",
    "\n",
    "**ATENÇÃO**: neste exercício temos uma coincidência: \n",
    "\n",
    "- A classe $0$ corresponde ao índice 0 no vetor de classes.\n",
    "- A classe $1$ corresponde ao índice 1 no vetor de classes.\n",
    "- ... e assim por diante.\n",
    "\n",
    "Trata-se de uma coincidência! Se as classes fossem \"cadeira\", \"mesa\" e \"chapeu\" essa coincidência não existiria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se você tem um classificador binário qualquer (você fez o seu próprio classificador, por exemplo) e quer usá-lo em classificação multiclasse, o scikit-learn já tem classes auxiliares para transformar seu classificador binário em OvO ou OvA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "\n",
    "ovo_clf = OneVsOneClassifier(\n",
    "    SGDClassifier(\n",
    "        max_iter=5,\n",
    "        tol=-np.inf,\n",
    "        random_state=RANDOM_SEED,\n",
    "    ))\n",
    "ovo_clf.fit(X_train, y_train)\n",
    "ovo_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ovo_clf.estimators_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de desempenho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim como no caso dos classificadores binários, estamos interessados em estimar a performance real dos nossos classificadores multiclasse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(\n",
    "    sgd_clf,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nada mal: um classificador aleatório teria apenas um desempenho médio de 10% de acurácia!\n",
    "\n",
    "Podemos melhorar o desempenho do classificador usando todos os truques das aulas passadas (GridSearch, scaling, data augmentation, etc). Por exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\n",
    "cross_val_score(\n",
    "    sgd_clf,\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de erro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bom, você já usou todos os truques básicos do nosso arsenal, e agora tem um modelo que é o melhor que você conseguiu até o momento. Para avançar mais, temos que mergulhar mais a fundo na análise dos erros que nosso classificador está fazendo.\n",
    "\n",
    "Uma primeira abordagem para a análise fina dos erros é a matriz de confusão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = cross_val_predict(\n",
    "    sgd_clf,\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "conf_mx = confusion_matrix(y_train, y_train_pred)\n",
    "conf_mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma visualização gráfica pode ser mais efetiva:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(np.log(1 + conf_mx), cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como esperado, a maior parte das predições é correta, e portanto a diagonal da matriz de confusão se sobressai. Para ressaltar os erros, vamos fazer o seguinte:\n",
    "\n",
    "- Normalizar as linhas pela soma dos valores da linha.\n",
    "- Zerar os elementos da diagonal, para facilitar a visualização dos erros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx / row_sums\n",
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "\n",
    "plt.matshow(np.log(1 + norm_conf_mx), cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora está mais claro: parece que temos muitos $5$ que são classificados como $3$ e vice-versa! Temos também vários $5$ classificados como $8$, mas o reverso é menos presente.\n",
    "\n",
    "Pode ser ilustrativo observar alguns exemplos específicos de erro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_a, cl_b = 3, 5\n",
    "X_aa = X_train[(y_train == cl_a) & (y_train_pred == cl_a)]\n",
    "X_ab = X_train[(y_train == cl_a) & (y_train_pred == cl_b)]\n",
    "X_ba = X_train[(y_train == cl_b) & (y_train_pred == cl_a)]\n",
    "X_bb = X_train[(y_train == cl_b) & (y_train_pred == cl_b)]\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(221)\n",
    "plot_digits(X_aa[:25], images_per_row=5)\n",
    "plt.subplot(222)\n",
    "plot_digits(X_ab[:25], images_per_row=5)\n",
    "plt.subplot(223)\n",
    "plot_digits(X_ba[:25], images_per_row=5)\n",
    "plt.subplot(224)\n",
    "plot_digits(X_bb[:25], images_per_row=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alguns erros são compreensíveis (digitos que de fato se parecem tanto com 3 como com 5), outros exemplos são misteriosos, como alguns digitos que são claramente 5 e foram classificados como 3. Por que um dígito que tão claramente se parece com um 5 foi classificado como 3? \n",
    "\n",
    "A resposta está no tipo de classificador usado. O SGDClassifier é um modelo linear: uma mera ponderação linear dos valores dos píxels. Se dois dígitos diferem apenas por poucos pixels, é fácil confundí-los."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade**: Agora que você tem um modelo treinado, avalie a acurária do modelo no conjunto de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividades:** Faça os problemas 1 e 2 do capítulo 3 do livro texto (Géron)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Desafio:** Problema 4 do livro texto (Géron)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
