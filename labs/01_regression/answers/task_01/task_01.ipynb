{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Um projeto de *machine learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objetivos:**\n",
    "\n",
    "- Aprender sobre o processo CRISP-DM;\n",
    "- Aplicar o CRISP-DM a um projeto real de *machine learning*;\n",
    "- Praticar análise exploratória;\n",
    "- Construir *pipelines* de processamento de dados em scikit-learn;\n",
    "- Entender o processo de construção, escolha e avaliação de modelos de *machine learning*;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *California Housing*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos trabalhar com um *dataset* de imóveis residenciais da Califórnia nos anos 1990."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtendo os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path.cwd().parents[3] / 'datasets' / 'housing'\n",
    "print(f'Saving data to {DATA_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "from urllib import request\n",
    "\n",
    "\n",
    "def fetch_housing_data(data_dir: Path) -> None:\n",
    "    '''Downloads the California Housing Prices dataset.\n",
    "\n",
    "    Downloads the California Housing Prices dataset from Aurelien Geron's\n",
    "    GitHub repository and saves it to the specified directory.\n",
    "\n",
    "    Args:\n",
    "        data_dir: The directory to which the dataset will be saved.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    if not data_dir.exists():\n",
    "        data_dir.mkdir(parents=True)\n",
    "\n",
    "    # Fetch the housing data.\n",
    "    HOUSING_URL = ('https://raw.githubusercontent.com/ageron/handson-ml2/'\n",
    "                   'master/datasets/housing/housing.tgz')\n",
    "    tgz_path = data_dir / 'housing.tgz'\n",
    "    request.urlretrieve(HOUSING_URL, tgz_path)\n",
    "\n",
    "    # Extract the housing data.\n",
    "    with tarfile.open(tgz_path) as housing_tgz:\n",
    "        housing_tgz.extractall(path=data_dir, filter='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_housing_data(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_housing_data(data_dir: Path) -> pd.DataFrame:\n",
    "    '''Loads the California Housing Prices dataset.\n",
    "\n",
    "    Loads the California Housing Prices dataset from the specified directory.\n",
    "\n",
    "    Args:\n",
    "        data_dir: The directory from which the dataset will be loaded.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the California Housing Prices dataset.\n",
    "    '''\n",
    "    csv_path = data_dir / 'housing.csv'\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_housing_data(DATA_DIR)\n",
    "\n",
    "print(f'O dataset tem {data.shape[0]} linhas e {data.shape[1]} colunas.')\n",
    "print('As colunas são:')\n",
    "for column_name in data.columns:\n",
    "    print(f'- \"{column_name}\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entendimento do negócio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esses dados representam informações censitárias acerca de *distritos* residenciais no estado da California na década de 1990.\n",
    "\n",
    "Atividade: Baseado nos nomes das colunas, você conseguiria escrever o significado de cada coluna?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resposta: não muito bem.\n",
    "\n",
    "Consultando o livro-texto da disciplina, capítulo 2, aprendemos que esses dados se referem aos distritos (conforme escrito no enunciado) e, portanto, são um pouco estranhos. Provavelmente significam, para cada distrito, o seguinte:\n",
    "\n",
    "- `longitude`: a longitude do centro;\n",
    "- `latitude`: a latitude do centro;\n",
    "- `housing_median_age`: a idade mediana dos imóveis;\n",
    "- `total_rooms`: essa é estranha, é a quantidade **total** de cômodos no distrito. Ou seja, a soma do número de cômodos de todos os imóveis;\n",
    "- `total_bedrooms`: a soma do número de *quartos* de todos os imóveis;\n",
    "- `population`: quantas pessoas moram no distrito;\n",
    "- `households`: número de imóveis;\n",
    "- `median_income`: renda mediana dos moradores do distrito;\n",
    "- `median_house_value`: o valor mediano dos imóveis do distrito. Esta é a nossa variável a ser predita;\n",
    "- `ocean_proximity`: só pelo nome é difícil saber a natureza exata desta variável. Trata-se de uma variável *categórica* indicando o \"status\" do distrito em relação à sua proximidade com o oceano Pacífico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atividade: Escreva o objetivo de negócios deste projeto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso objetivo é prever o valor mediano dos imóveis de um distrito residencial da Califórnia, baseado em uma série de atributos deste conforme visto acima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise exploratória"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo da análise exploratória é \"conhecer\" os dados:\n",
    "\n",
    "- Qual a distribuição de cada *feature*?\n",
    "\n",
    "- Qual a natureza de cada *feature*?\n",
    "\n",
    "    - Unidade de medida\n",
    "\n",
    "    - Se é estritamente positiva ou se pode ser positiva ou negativa\n",
    "\n",
    "    - Para que serve?\n",
    "\n",
    "- Quais e como são as distribuições conjuntas de *features*? Em particular, como as *features* se relacionam com o *target*?\n",
    "\n",
    "- Existem anomalias e erros?\n",
    "\n",
    "    - Dados faltantes\n",
    "\n",
    "    - \"Saturação\" de dados\n",
    "\n",
    "    - Outliers\n",
    "\n",
    "    - Desbalanceamento de classes\n",
    "\n",
    "    - Dados duplicados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análise exploratória: antes ou depois da separação treino-teste?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante o processo de modelagem vamos dividir os dados em dois conjuntos: \"dados de treino\" e \"dados de teste\". Devemos treinar nossos modelos com o conjunto de dados de treino, e avaliar seu desempenho no conjunto de teste, para que não nos enganemos com desempenhos preditivos excelentes no conjunto de treino e que não se reproduzem no conjunto de teste!\n",
    "\n",
    "Atividade: Como se chama o fenômeno no qual temos um excelente desempenho no conjunto de treino e um desempenho bem menor no conjunto de teste?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<span style=\"color:red;font-family:Papyrus;font-weight:bold;font-size:30px\">\n",
    "    OVERFITTING!\n",
    "</span>\n",
    "<span style=\"color:darkgray;font-size:10px\">\n",
    "    booooooo!\n",
    "</span>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando devemos fazer a análise exploratória?\n",
    "\n",
    "- Antes da separação treino-teste, ou seja, no conjunto de dados completo?\n",
    "\n",
    "- Depois da separação treino-teste, ou seja, usando apenas o conjunto de dados de treino?\n",
    "\n",
    "Esta é uma pergunta difícil de responder.\n",
    "\n",
    "- Analisar antes da separação:\n",
    "\n",
    "    - Vantagens: todo o conjunto de dados de exemplo que foi coletado está á nossa disposição para estudo, o que torna mais fácil a detecção de anomalias raras, como outliers ou a ocorrência de categorias raras em *features* categóricas.\n",
    "\n",
    "    - Desvantagens: corremos o risco de \"data snooping\" (\"bisbilhotar\" os dados), onde acabamos por aprender algo sobre os dados que pode impactar de modo \"injusto\" nossa modelagem - é como se estivéssemos \"overfittando\" sem querer!\n",
    "\n",
    "- Analisar depois da separação:\n",
    "\n",
    "    - Vantagens: reduz o risco de \"data snooping\"\n",
    "\n",
    "    - Desvantagens: podemos não perceber anomalias e erros raros nos dados, que podem impactar nossa modelagem de uma forma que é difícil de identificar.\n",
    "\n",
    "O que fazer então? Em geral, queremos balancear o risco de \"data snooping\" com o risco de não entender os detalhes mais finos e raros dos dados. Portando a recomendação é fazer análises exploratórias antes e depois da separação, com objetivos diferentes:\n",
    "\n",
    "- Análise exploratória antes da separação: faça apenas análises globais e simples, para checar a sanidade dos dados e realizar filtragens simples. Evite análises que conectem as *features* com o *target*.\n",
    "\n",
    "- Análise exploratória depois da separação: você está livre para explorar o que quiser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos fazer uma primeira análise dos dados, apenas para checar a integridade destes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atividade: Acompanhe o desenvolvimento da análise exploratória a ser feito pelo professor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Nossa exploração terá três fases:\n",
    "\n",
    "- Análise exploratória preliminar\n",
    "- Separação treino-teste\n",
    "- Análise exploratória complementar\n",
    "\n",
    "É importante distinguir a análise exploratória realizada antes da separação de dados em conjuntos de treino e teste, daquela posterior a esta separação, para evitar o *\"data snooping\"* descrito acima.\n",
    "\n",
    "#### Análise exploratória\n",
    "\n",
    "Nesta fase vamos conhecer a natureza dos dados sem explorar suas interrelações, em especial não vamos explorar a conexão entre as *features* e o *target*.\n",
    "\n",
    "Vamos proceder da seguinte forma:\n",
    "\n",
    "<table>\n",
    "\n",
    "<tr>\n",
    "<th> Etapa </th>\n",
    "<th> Objetivos </th>\n",
    "<th> Ferramentas </th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "\n",
    "<td>\n",
    "    Uma análise global do volume de dados\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "    <ul>\n",
    "    <li> Quais são as <em>features</em>?</li>\n",
    "    <li> Quem é o <em>target</em>?</li>\n",
    "    <li> Quais variáveis são contínuas e quais são categóricas?</li>\n",
    "    <li> Existem dados faltantes? </li>\n",
    "    <li> Existem dados duplicados? </li>\n",
    "    </ul>\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "    <div>\n",
    "    <ul>\n",
    "    <li> Número de linhas e colunas </li>\n",
    "    <li> Tipo de dados de cada coluna </li>\n",
    "    <li> Remoção de linhas duplicadas <em>se isso realmente for um erro</em> </li>\n",
    "    </ul>\n",
    "    </div>\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "\n",
    "<td>\n",
    "    Uma primeira análise da natureza das features\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "    <ul>\n",
    "    <li> Qual a unidade de medida de cada variável? </li>\n",
    "    <li> Existem anomalias? \n",
    "        <ul>\n",
    "            <li> <em>outliers</em> </li>\n",
    "            <li> erros grosseiros </li>\n",
    "            <li> <em>spikes</em> </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li> Para cada variável contínua\n",
    "        <ul>\n",
    "            <li> Simétrica? </li>\n",
    "            <li> Estritamente positiva/negativa? </li>\n",
    "            <li> Unimodal / multimodal? </li>\n",
    "            <li> Cauda longa à direita/esquerda? </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li> Para cada variável categórica\n",
    "        <ul>\n",
    "            <li>Categorias raras?</li>\n",
    "            <li>Categorias dominantes?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    </ul>\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "    <ul>\n",
    "    <li>Medidas descritivas de posição e espalhamento</li>\n",
    "    <li>Histogramas</li>\n",
    "    <li>Tabelas de frequência</li>\n",
    "    <li>Gráficos de barra</li>\n",
    "    </ul>\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Análise global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos conferir os tipos de dados espiando as primeiras linhas do *dataset*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E vamos verificar quantos exemplos temos, e qual o tipo de dados identificado pelo Pandas para cada coluna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As colunas do tipo `float64` são todas contínuas neste exemplo. Isso nem sempre é verdade: pode ser que tenhamos uma variável categórica que foi lida como `float64` por engano! Sempre verifique a natureza das suas variáveis.\n",
    "\n",
    "Em teoria, não é estritamente correto dizer que essas variáveis `float64` são perfeitamente contínuas. Na verdade, a maioria delas é discreta, pois trata-se de contagens ou de idade inteira: `housing_median_age`, `total_rooms`, `total_bedrooms`, `population`, `households`. A decisão de convertê-las para inteiros ou mantê-las como números reais é dependente de aplicação. Para nós, neste projeto, não vai fazer diferença.\n",
    "\n",
    "A feature `ocean_proximity` é do tipo `object`, que é o tipo de dados que o Pandas associa a qualquer coisa que não seja um número."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Vamos registrar as descrições e unidades de medida de cada variável:\n",
    "\n",
    "| Variável             | Unidade de medida  | Descrição                                         |\n",
    "|----------------------|--------------------|---------------------------------------------------|\n",
    "| `longitude`          | graus              | a longitude do centro                             |\n",
    "| `latitude`           | graus              | a latitude do centro                              |\n",
    "| `housing_median_age` | anos               | a idade mediana dos imóveis                       |\n",
    "| `total_rooms`        | n/a - contagem     | soma do número de cômodos de todos os imóveis     |\n",
    "| `total_bedrooms`     | n/a - contagem     | a soma do número de *quartos* de todos os imóveis |\n",
    "| `population`         | n/a - contagem     | quantas pessoas moram no distrito                 |\n",
    "| `households`         | n/a - contagem     | número de imóveis                                 |\n",
    "| `median_income`      | US$ $\\times 10000$ | renda mediana dos moradores do distrito           |\n",
    "| `median_house_value` | US$                | o valor mediano dos imóveis do distrito           |\n",
    "| `ocean_proximity`    | n/a - categoria    | categoria de proximidade com o oceano Pacífico    |\n",
    "\n",
    "As informações aqui foram obtidas do livro-texto da disciplina, capítulo 2, que está servindo de \"Business Expert\" para a gente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_dups = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_dups.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O *dataset* sem linhas duplicadas tem o mesmo tamanho do *dataset* original, indicando que não existiam linhas duplicadas de qualquer forma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Análise preliminar das features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos verificar as medidas descritivas de cada feature, a começar pelas variáveis contínuas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_dtypes(include='number').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando a tabela fica muito longa horizontalmente e curta verticalmente, tente \"transpor\" a tabela para ver se fica melhor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_dtypes(include='number').describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melhorou. Agora limite o número de casas decimais para ficar mais fácil ainda de observar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_dtypes(include='number').describe().round(2).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos o seguinte:\n",
    "\n",
    "- Todas as colunas possuem $20640$ itens não-nulos (\"count\"), exceto pela coluna `total_bedrooms` que tem valores faltantes. Vamos discutir medidas para lidar com isso em breve.\n",
    "\n",
    "- Latitude e Longitude ficam em torno de $-120\\degree$ e $36\\degree$, correspondendo à localização geral da Califórnia.\n",
    "\n",
    "<center>\n",
    "<img \n",
    "    src=\"../../../../resources/california_location.png\"\n",
    "    alt=\"Snapshot of California showing latitude and longitude\"\n",
    "    width=\"400\"\n",
    "/>\n",
    "</center>\n",
    "\n",
    "- Fora latitude e longitude, as features são todas estritamente positivas, tem valor mínimo teórico de zero, e tem uma variabilidade muito alta! Podemos analisar a magnitude da variabilidade de cada feature em relação à sua própria média, obtendo desta forma um valor normalizado de variação chamado *coeficiente de variação*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coefficient_of_variation(data: pd.DataFrame) -> pd.Series:\n",
    "    '''Computes the coefficient of variation for each column in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data: A pandas DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        A pandas Series containing the coefficient of variation for each column\n",
    "        in the input DataFrame.\n",
    "    '''\n",
    "    stats = data \\\n",
    "        .select_dtypes(include='number') \\\n",
    "        .describe() \\\n",
    "        .transpose() \\\n",
    "        .drop(['longitude', 'latitude'], axis=0)\n",
    "    CV = stats['std'] / stats['mean']\n",
    "    CV.rename('Coefficient of Variation', inplace=True)\n",
    "    return CV\n",
    "\n",
    "\n",
    "print(compute_coefficient_of_variation(data).round(2).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature            |   Coefficient of Variation |\n",
    "|:-------------------|---------------------------:|\n",
    "| housing_median_age |                       0.44 |\n",
    "| total_rooms        |                       0.83 |\n",
    "| total_bedrooms     |                       0.78 |\n",
    "| population         |                       0.79 |\n",
    "| households         |                       0.77 |\n",
    "| median_income      |                       0.49 |\n",
    "| median_house_value |                       0.56 |\n",
    "\n",
    "Podemos observar que as medidas de desvio padrão, quando normalizadas pelo valor da média (ou seja, o coeficiente de variação), são valores entre $44\\%$ e $83\\%$, indicando que existe muita diferença entre os distritos da Califórnia em todos esses atributos. \n",
    "\n",
    "E daí? Bem, esta observação pode indicar alternativas de modelagem dos dados. Talvez existam classes intrínsecas de distritos californianos, e que modelar essa possível *variável latente* pode ser útil para melhorar o desempenho do modelo. Talvez tenhamos *outliers* e cauda longa, que estão enviesando o cálculo do desvio padrão. De qualquer modo, indica que devemos prestar mais atenção na distribuição dos dados.\n",
    "\n",
    "Vamos então estudar a distribuição de cada variável com histogramas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Regra prática: número de bins = sqrt(número de amostras)\n",
    "n_bins = np.floor(np.sqrt(data.shape[0])).astype(int).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "measurement_units = {\n",
    "    'longitude': 'graus',\n",
    "    'latitude': 'graus',\n",
    "    'housing_median_age': 'anos',\n",
    "    'total_rooms': 'cômodos',\n",
    "    'total_bedrooms': 'quartos',\n",
    "    'population': 'pessoas',\n",
    "    'households': 'imóveis',\n",
    "    'median_income': r'USD $\\times 10000$',\n",
    "    'median_house_value': r'USD',\n",
    "    'ocean_proximity': 'classe',\n",
    "}\n",
    "\n",
    "result = data \\\n",
    "    .select_dtypes(include='number') \\\n",
    "    .hist(bins=n_bins, figsize=(20, 15))\n",
    "\n",
    "for subplot in result.flatten():\n",
    "    column = subplot.get_title()\n",
    "    if not column:\n",
    "        continue\n",
    "    unit = measurement_units[column]\n",
    "    subplot.set_xlabel(unit)\n",
    "    subplot.set_ylabel('Frequência')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos o seguinte:\n",
    "\n",
    "**Latitude e longitude**\n",
    "\n",
    "Estão flutuando de um jeito dificil de entender. Mas isso é esperado - basta plotar os dados e compará-los á um mapa da California:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_geo_pop():\n",
    "    '''Plots the geographical distribution of the population in California.'''\n",
    "    # Ordena os dados por população. É um truque para evitar que pontos\n",
    "    # sobrepostos ocultem outros pontos. Assim, os pontos com populações\n",
    "    # maiores são plotados por último.\n",
    "    sorted_data = data.sort_values(by='population', ascending=True)\n",
    "\n",
    "    longitude = sorted_data['longitude']\n",
    "    latitude = sorted_data['latitude']\n",
    "    population = sorted_data['population']\n",
    "\n",
    "    # Ajusta a razão de aspecto para compensar a distorção da projeção.\n",
    "    aspect_ratio = 1 / np.cos(np.mean(latitude) * np.pi / 180)\n",
    "\n",
    "    img = plt.imread(Path.cwd().parents[1] / 'resources' / 'california.png')\n",
    "\n",
    "    n_rows, n_cols = 1, 2\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(16, 8))\n",
    "\n",
    "    ax = axes[0]\n",
    "    sct = ax.scatter(\n",
    "        longitude,\n",
    "        latitude,\n",
    "        alpha=0.5,\n",
    "        s=population / 200,\n",
    "        c=population,\n",
    "        cmap='viridis',\n",
    "    )\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    ax.set_aspect(aspect_ratio)\n",
    "\n",
    "    ax = axes[1]\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "\n",
    "    fig.suptitle('Distribuição Geográfica da População na Califórnia')\n",
    "    fig.colorbar(\n",
    "        sct,\n",
    "        ax=axes[0],\n",
    "        orientation='vertical',\n",
    "        label='População por distrito',\n",
    "    )\n",
    "    fig.tight_layout()\n",
    "\n",
    "\n",
    "plot_geo_pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aparentemente a população californiana se distribui heterogeneamente, como se espera de qualquer população."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Spikes***\n",
    "\n",
    "As colunas `house_median_age`, `median_house_value` e `median_income` possuem **spikes** em seus histogramas: barras finas e proeminentes no gráfico. Isso indica um excesso de elementos no *dataset* que apresentam o valor do *spike*, e requer investigação quanto á suas causas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'median_income',\n",
    "    'housing_median_age',\n",
    "    'median_house_value',\n",
    "]\n",
    "\n",
    "spike_locations = {\n",
    "    'median_income': 15,\n",
    "    'housing_median_age': 52,\n",
    "    'median_house_value': 500001,\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(3, 1)\n",
    "fig.set_size_inches(8, 12)\n",
    "for ax, column in zip(axes, columns):\n",
    "    ax.hist(data[column], bins=n_bins, density=True)\n",
    "    ax.axvline(\n",
    "        spike_locations[column],\n",
    "        color='red',\n",
    "        linestyle='--',\n",
    "        linewidth=10,\n",
    "        alpha=0.3,\n",
    "    )\n",
    "    ax.set_xlabel(measurement_units[column])\n",
    "    ax.set_ylabel('Frequência')\n",
    "    ax.set_title(column)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste caso, como os *spikes* estão localizados no extremo direito do histograma, provavelmente são devidos a um efeito de *saturação*. Por exemplo, pode ter sido o caso que a variável `median_house_value` registrou todos os preços de imóveis corretamente, exceto para imóveis que valiam mais do que USD $500$ mil - situação na qual o valor registrado é simplesmente $500001$.\n",
    "\n",
    "O que fazer nesse caso? Posto que trata-se de dados inválidos, o melhor pode ser simplesmente ignorar esses exemplos - ou seja, removê-los do dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Colunas positivas com cauda à direita**\n",
    "\n",
    "As colunas `total_rooms`, `total_bedrooms`, `population`, `households`, `median_income` e `median_house_value` apresentam um comportamento comum:\n",
    "\n",
    "- São quantidades estritamente positivas;\n",
    "- Possuem cauda longa à direita.\n",
    "\n",
    "Nestes casos, pode ser interessante aplicar uma transformação do tipo \"logaritmo\" aos dados. Vejamos como fica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'total_rooms',\n",
    "    'total_bedrooms',\n",
    "    'population',\n",
    "    'households',\n",
    "    'median_income',\n",
    "    'median_house_value',\n",
    "]\n",
    "\n",
    "results = data[columns] \\\n",
    "    .map(np.log10) \\\n",
    "    .hist(bins=n_bins, figsize=(20, 15))\n",
    "\n",
    "for subplot in results.flatten():\n",
    "    column = subplot.get_title()\n",
    "    if not column:\n",
    "        continue\n",
    "    unit = measurement_units[column]\n",
    "    subplot.set_xlabel('$\\\\log_{10}$ ' + f'({unit})')\n",
    "    subplot.set_ylabel('Frequência')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É interessante notar que os dados, após uma transformação logarítmica, passam a ter uma característica mais simétrica, e sem *outliers*! Isso pode ser muito benéfico para alguns tipos de modelos, em especial o *modelo linear* que veremos em breve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora analisar a única *feature* categórica:\n",
    "\n",
    "**`ocean_proximity`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = data['ocean_proximity'].value_counts()\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.plot.barh()\n",
    "plt.xlabel('Frequência')\n",
    "plt.ylabel('Classe')\n",
    "plt.title('Frequência de Classes de Proximidade ao Oceano')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos uma classe com pouquíssima representação: `ISLAND`. Temos que decidir o que fazer: remover esses casos ou não? Para simplificar a análise, vamos remover esses casos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtragem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atividade: Baseado no que aprendemos na análise exploratória, escreva um código para filtrar o *dataset*. Não modifique as colunas, apenas aceite ou rejeite cada linha de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_rows(data: pd.DataFrame) -> pd.Series:\n",
    "    '''Returns a boolean Series indicating which rows of a DataFrame are valid.\n",
    "\n",
    "    Args:\n",
    "        data: A pandas DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        A boolean Series indicating which rows of the input DataFrame are valid.\n",
    "    '''\n",
    "    # Remove rows with spikes.\n",
    "    valid_rows = ((data['median_income'] < 15) &\n",
    "                  (data['housing_median_age'] < 52) &\n",
    "                  (data['median_house_value'] < 500001))\n",
    "\n",
    "    # Remove rows with ocean_proximity == 'ISLAND'.\n",
    "    valid_rows &= data['ocean_proximity'] != 'ISLAND'\n",
    "\n",
    "    return valid_rows\n",
    "\n",
    "\n",
    "def filter_data(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''Filters a DataFrame to remove invalid rows.\n",
    "\n",
    "    Args:\n",
    "        data: A pandas DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with invalid rows removed.\n",
    "    '''\n",
    "    valid_rows = get_valid_rows(data)\n",
    "    return data[valid_rows]\n",
    "\n",
    "\n",
    "filtered_data = filter_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separação treino-teste\n",
    "\n",
    "Antes de prosseguir com nossa análise exploratória, é melhor realizar a *separação treino-teste*, na qual vamos reservar uma porção dos nossos dados de exemplo para *treinar* nossos modelos, e outra parcela para *testar* o modelo escolhido e ter uma melhor noção acerca do real desempenho deste. \n",
    "\n",
    "Por que avaliar em um conjunto de dados diferente daquele de treino? Bem, em primeiro lugar, devemos lembrar que ambos os conjuntos de dados (treino e teste) vieram do mesmo conjunto de exemplos, e portanto são amostras da mesma população. Logo, ao avaliar o desempenho do modelo escolhido em um conjunto de teste, estamos verificando o comportamento estimado do modelo na população em geral.\n",
    "\n",
    "Mas o conjunto de treino também veio da população, por que ele não serve para fazer uma avaliação de desempenho final? Por causa do \n",
    "\n",
    "<center>\n",
    "<span style=\"color:red;font-family:Papyrus;font-weight:bold;font-size:30px\">\n",
    "    OVERFITTING!\n",
    "</span>\n",
    "<span style=\"color:darkgray;font-size:10px\">\n",
    "    booooooo!\n",
    "</span>\n",
    "</center>\n",
    "\n",
    "Pode ser que seu modelo tenha se ajustado a características irrelevantes dos dados (\"ruído\") ao invés de se concentrar no comportamento mais provável dos seus dados.\n",
    "\n",
    "Em termos semi-matemáticos, a idéia é a seguinte:\n",
    "\n",
    "- Seus dados observados são uma mistura de dados \"reais\" DETERMINÍSTICOS (entra um certo \"$x$\", sai sempre o mesmo \"y\") e ruído ALEATÓRIO (cada vez que mede dá diferente);\n",
    "\n",
    "- Para a parte determinística, estamos supondo que *realmente* existe uma parte $y = f(x)$ nesse experimento;\n",
    "\n",
    "<center><img src=\"../../../../resources/y_sinal.png\" width=600/></center>\n",
    "\n",
    "- Para a parte aleatória, estamos supondo que trata-se de valores obtidos de uma distribuição de média zero, e que são independentes entre observações.\n",
    "\n",
    "<center><img src=\"../../../../resources/y_ruido.png\" width=600/></center>\n",
    "\n",
    "- Os valores observados são, portanto, uma combinação do valor \"real\" (que só a fada dos sinais conhece) e do \"ruído\" (também, só de conhecimento do coelhinho da Páscoa dos ruídos), gerando o valor observado por nós.\n",
    "\n",
    "<center><img src=\"../../../../resources/y_observado.png\" width=600/></center>\n",
    "\n",
    "- Entre duas amostras separadas de observações da população (o \"conjunto de treino\" e o \"conjunto de teste\") temos a repetição do comportamento da parte DETERMINÍSTICA, no sentido de que a regra de geração de valores $y$ a partir de $x$ (ou seja, a componente \"escondida\" $y = f(x)$ está firme e forte), e a parte ALEATÓRIA é a imprevisível.\n",
    "\n",
    "<center><img src=\"../../../../resources/y_treino_teste.png\" width=600/></center>\n",
    "\n",
    "Ao treinar com\n",
    "\n",
    "<center>\n",
    "<span style=\"color:red;font-family:Papyrus;font-weight:bold;font-size:30px\">\n",
    "    OVERFITTING!\n",
    "</span>\n",
    "<span style=\"color:darkgray;font-size:10px\">\n",
    "    booooooo!\n",
    "</span>\n",
    "</center>\n",
    "\n",
    "a gente acaba aprendendo um $y = g(x)$ onde $g(x)$ incorpora o $f(x)$ (alegria!) e também um extra ali que é só o ruido OBSERVADO NAQUELE CONJUNTO DE TREINO ESPECÌFICO! O modelo não sabe disso, tudo que ele observa é o conjunto de treino, sem saber o que é que daquilo ali é ruido!\n",
    "\n",
    "<center><img src=\"../../../../resources/erro_overfitting.png\" width=600/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(\n",
    "    filtered_data,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=filtered_data['ocean_proximity'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos prosseguir com a análise exploratória.\n",
    "\n",
    "## Análise exploratória pós-separação treino-teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_features_target(\n",
    "    train_data: pd.DataFrame, test_data: pd.DataFrame\n",
    ") -> tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n",
    "    '''Splits the features and target from the training and test sets.\n",
    "\n",
    "    Args:\n",
    "        train_data: The training set.\n",
    "        test_data: The test set.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the features and target for the training set, followed\n",
    "        by the features and target for the test set.\n",
    "    '''\n",
    "    X_train = train_data.drop('median_house_value', axis=1).copy()\n",
    "    y_train = train_data['median_house_value'].copy()\n",
    "\n",
    "    X_test = test_data.drop('median_house_value', axis=1).copy()\n",
    "    y_test = test_data['median_house_value'].copy()\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = split_features_target(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise de correlações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlações entre features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlações entre features contínuas**\n",
    "\n",
    "Vamos verificar como as *features* contínuas se inter-relacionam usando diagramas de espalhamento e métricas de correlação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = X_train.select_dtypes(include='number').corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,\n",
    "    cmap='coolwarm',\n",
    "    fmt='.2f',\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aparentemente as colunas `total_rooms`, `total_bedrooms`, `population` e `households` são altamente correlacionadas, o que faz sentido: quanto mais imóveis, mais cômodos, quartos e gente. Geralmente não gostamos de *features* muito correlacionadas: isto pode ser um sinal de que a verdadeira informação está escondida por trás destas *features*, que são apenas o reflexo desta informação escondida.\n",
    "\n",
    "Vamos ver o que acontece se dividimos `total_rooms`, `total_bedrooms` e `population` por `households`, gerando assim novas *features*. Vamos também remover as *features* `total_rooms`, `total_bedrooms` e `population`. Note que não estamos perdendo informação: com as novas *features* podemos reconstruir as antigas *features* quando quisermos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new_features = X_train.copy()\n",
    "\n",
    "X_train_new_features['rooms_per_household'] = \\\n",
    "    X_train['total_rooms'] / X_train['households']\n",
    "X_train_new_features['bedrooms_per_household'] = \\\n",
    "    X_train['total_bedrooms'] / X_train['households']\n",
    "X_train_new_features['population_per_household'] = \\\n",
    "    X_train['population'] / X_train['households']\n",
    "\n",
    "X_train_new_features.drop(\n",
    "    columns=['total_rooms', 'total_bedrooms', 'population'],\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "corr_matrix = X_train_new_features.select_dtypes(include='number').corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,\n",
    "    cmap='coolwarm',\n",
    "    fmt='.2f',\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que `rooms_per_household` e `bedrooms_per_household` ainda são muito correlacionadas, vamos criar `bedrooms_per_room` e ver o que acontece. Vamos também remover `bedrooms_per_household` (novamente, sem perda de informação: de `households`, `rooms_per_household` e `bedrooms_per_room` podemos recriar a informação removida)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new_features['bedrooms_per_room'] = \\\n",
    "    X_train['total_bedrooms'] / X_train['total_rooms']\n",
    "\n",
    "X_train_new_features.drop(columns=['bedrooms_per_household'], inplace=True)\n",
    "\n",
    "corr_matrix = X_train_new_features.select_dtypes(include='number').corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,\n",
    "    cmap='coolwarm',\n",
    "    fmt='.2f',\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, as colunas `latitude` e `longitude` são altamente correlacionadas pois a California é um estado mais ou menos \"esticado\". Nada para se fazer por aqui que não seja mais sofisticado (e.g. análise de componentes principais) para o momento.\n",
    "\n",
    "Vamos agora analisar os histogramas das features resultantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_units = {\n",
    "    'longitude': 'graus',\n",
    "    'latitude': 'graus',\n",
    "    'housing_median_age': 'anos',\n",
    "    'households': 'imóveis',\n",
    "    'median_income': r'USD $\\times 10000$',\n",
    "    'rooms_per_household': 'cômodos/imóvel',\n",
    "    'population_per_household': 'pessoas/imóvel',\n",
    "    'bedrooms_per_room': 'quartos/cômodo',\n",
    "}\n",
    "\n",
    "result = X_train_new_features \\\n",
    "    .select_dtypes(include='number') \\\n",
    "    .hist(bins=n_bins, figsize=(20, 15))\n",
    "\n",
    "for subplot in result.flatten():\n",
    "    column = subplot.get_title()\n",
    "    if not column:\n",
    "        continue\n",
    "\n",
    "    unit = measurement_units[column]\n",
    "    subplot.set_xlabel(unit)\n",
    "    subplot.set_ylabel('Frequência')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new_features \\\n",
    "    .select_dtypes(include='number') \\\n",
    "    .describe() \\\n",
    "    .round(2) \\\n",
    "    .transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epa, tem coisa estranha aqui! Como assim temos distritos com 600 pessoas por imóvel! Ou 60 cômodos por imóvel! Algo está muito estranho!\n",
    "\n",
    "Vamos ver o que anda acontecendo com a variável `population_per_household`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new_features['population_per_household'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new_features \\\n",
    "    .sort_values(by='population_per_household', ascending=False) \\\n",
    "    .head(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver **onde** estes distritos estranhos estão - afinal temos a latitude e a longitude! Uma rápida consulta a uma aplicação de mapas revela o seguinte:\n",
    "\n",
    "- $\\text{latitude} = 40.41, \\text{longitude} = -120.51$:\n",
    "\n",
    "<center>\n",
    "<img src=\"../../../../resources/pop_outlier_1.png\" width=600 title=\"High Desert State Prison\"/>\n",
    "</center>\n",
    "\n",
    "Parece que nesta \"casa\" tem muita gente mesmo. Melhor remover do conjunto de dados.\n",
    "\n",
    "- $\\text{latitude} = 33.97, \\text{longitude} = -117.33$:\n",
    "\n",
    "<center>\n",
    "<img src=\"../../resources/pop_outlier_2.png\" width=600 title=\"University of California, Riverside\"/>\n",
    "</center>\n",
    "\n",
    "Aqui já não consigo explicar, mas parece ser um erro também. Os mais cínicos dirão que é do mesmo tipo da \"casa\" anterior, mas isso é veneno da língua só.\n",
    "\n",
    "- $\\text{latitude} = 33.94, \\text{longitude} = -117.63$:\n",
    "\n",
    "<center>\n",
    "<img src=\"../../../../resources/pop_outlier_3.png\" width=600 title=\"California Institution for Women, uma prisão feminina\"/>\n",
    "</center>\n",
    "\n",
    "Mais uma prisão, melhor remover.\n",
    "\n",
    "- $\\text{latitude} = 32.56, \\text{longitude} = -116.97$:\n",
    "\n",
    "<center>\n",
    "<img src=\"../../../../resources/pop_outlier_4.png\" width=600 title=\"Southwestern College\"/>\n",
    "</center>\n",
    "\n",
    "Novamente, o melhor que eu posso explicar aqui é a presença de uma instituição de ensino superior, de resto não faço idéia. Mas não parece ser um \"típico\" bairro residencial, podemos excluir da análise.\n",
    "\n",
    "Em geral, parece que locais com número de imóveis muito reduzido são anômalos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.text import Text\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "def plot_scatter(\n",
    "    df: pd.DataFrame,\n",
    "    x: str,\n",
    "    y: str,\n",
    "    x_label: str,\n",
    "    y_label: str,\n",
    "    x_lim: tuple[float, float] | None,\n",
    "    y_lim: tuple[float, float] | None,\n",
    "    title: str,\n",
    "    inches_per_col: float = 5,\n",
    "    aspect_ratio: float = 4.0 / 3.0,\n",
    ") -> None:\n",
    "    '''Plots the population per household as a function of the number of households.'''\n",
    "\n",
    "    def _set_endpoints(\n",
    "        ticks: Iterable[float],\n",
    "        endpoints: tuple[float | None, float | None],\n",
    "    ) -> None:\n",
    "        if endpoints[0] is not None:\n",
    "            ticks[0] = endpoints[0]\n",
    "        if endpoints[1] is not None:\n",
    "            ticks[-1] = endpoints[1]\n",
    "        return ticks\n",
    "\n",
    "    def _set_endpoint_xticks(\n",
    "        ax: plt.Axes,\n",
    "        endpoints: tuple[float, float],\n",
    "    ) -> None:\n",
    "        ticks = ax.get_xticks()\n",
    "        ticks = _set_endpoints(ticks, endpoints)\n",
    "        ax.set_xticks(ticks)\n",
    "\n",
    "    def _set_endpoint_yticks(\n",
    "        ax: plt.Axes,\n",
    "        endpoints: tuple[float, float],\n",
    "    ) -> None:\n",
    "        ticks = ax.get_yticks()\n",
    "        ticks = _set_endpoints(ticks, endpoints)\n",
    "        ax.set_yticks(ticks)\n",
    "\n",
    "    def _plot_scatter(\n",
    "        ax: plt.Axes,\n",
    "        use_log: bool,\n",
    "        x_lim: tuple[float, float] | None,\n",
    "        y_lim: tuple[float, float] | None,\n",
    "    ) -> None:\n",
    "        df.plot.scatter(x=x, y=y, ax=ax, alpha=0.3)\n",
    "        ax.set_xlabel(x_label)\n",
    "        ax.set_ylabel(y_label)\n",
    "        if use_log:\n",
    "            ax.set_xscale('log')\n",
    "            ax.set_yscale('log')\n",
    "        if x_lim is not None:\n",
    "            _set_endpoint_xticks(ax, x_lim)\n",
    "            ax.set_xlim(x_lim)\n",
    "        if y_lim is not None:\n",
    "            _set_endpoint_yticks(ax, y_lim)\n",
    "            ax.set_ylim(y_lim)\n",
    "\n",
    "    n_rows = 3\n",
    "    n_cols = 2\n",
    "\n",
    "    width = n_cols * inches_per_col\n",
    "    height = n_rows * inches_per_col / aspect_ratio\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=n_rows,\n",
    "        ncols=n_cols,\n",
    "        figsize=(width, height),\n",
    "    )\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    fig.tight_layout(h_pad=4.0, w_pad=4.0)\n",
    "\n",
    "    _plot_scatter(\n",
    "        ax=axes[0, 0],\n",
    "        use_log=False,\n",
    "        x_lim=None,\n",
    "        y_lim=None,\n",
    "    )\n",
    "    _plot_scatter(\n",
    "        ax=axes[0, 1],\n",
    "        use_log=True,\n",
    "        x_lim=None,\n",
    "        y_lim=None,\n",
    "    )\n",
    "    _plot_scatter(\n",
    "        ax=axes[1, 0],\n",
    "        use_log=False,\n",
    "        x_lim=None,\n",
    "        y_lim=y_lim,\n",
    "    )\n",
    "    _plot_scatter(\n",
    "        ax=axes[1, 1],\n",
    "        use_log=True,\n",
    "        x_lim=None,\n",
    "        y_lim=y_lim,\n",
    "    )\n",
    "    _plot_scatter(\n",
    "        ax=axes[2, 0],\n",
    "        use_log=False,\n",
    "        x_lim=x_lim,\n",
    "        y_lim=y_lim,\n",
    "    )\n",
    "    _plot_scatter(\n",
    "        ax=axes[2, 1],\n",
    "        use_log=True,\n",
    "        x_lim=x_lim,\n",
    "        y_lim=y_lim,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(\n",
    "    df=X_train_new_features,\n",
    "    x='households',\n",
    "    y='population_per_household',\n",
    "    x_label='Número de Imóveis',\n",
    "    y_label='População por Imóvel',\n",
    "    x_lim=(50, None),\n",
    "    y_lim=(1, 15),\n",
    "    title='População por Imóvel em Função do Número de Imóveis',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando o gráfico acima, podemos ver que:\n",
    "\n",
    "- As anomalias de \"população por imóvel\" ocorrem para valores baixos de número de imóveis no distrito. \n",
    "- Ao utilizar a escala logarítmica para os gráficos de espalhamento, observamos que estes passam a se relacionar de forma mais \"gaussiana\", o que pode ser vantajoso para alguns modelos de machine learning. Quais modelos? Bem, é por isso que vamos aprender sobre os modelos em detalhe mais pra frente. Por enquanto, posso adiantar que modelos lineares podem se beneficiar de uma maior \"normalidade\" dos dados.\n",
    "\n",
    "Vamos repetir esta análise para `households` e `rooms_per_household`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(\n",
    "    df=X_train_new_features,\n",
    "    x='households',\n",
    "    y='rooms_per_household',\n",
    "    x_label='Número de Imóveis',\n",
    "    y_label='Cômodos por Imóvel',\n",
    "    x_lim=None,\n",
    "    y_lim=(1, 15),\n",
    "    title='Cômodos por Imóvel em Função do Número de Imóveis',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui temos esta anomalia de \"cômodos por imóvel\", mas é menos severa do que o caso de \"população por imóvel\". Ainda assim, observamos que acontece para distritos com baixo número de imóveis. Isto é mais uma evidência para cortar os distritos com poucos imóveis, embora seja difícil determinar um bom ponto de corte.\n",
    "\n",
    "Vamos repetir a análise agora para as variáveis `households` e `bedrooms_per_room`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(\n",
    "    df=X_train_new_features,\n",
    "    x='households',\n",
    "    y='bedrooms_per_room',\n",
    "    x_label='Número de Imóveis',\n",
    "    y_label='Quartos por Cômodo',\n",
    "    x_lim=None,\n",
    "    y_lim=(0.1, 1),\n",
    "    title='Quartos por Cômodo em Função do Número de Imóveis',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Novamente, um pouco de *outliers* aqui para valores pequenos de número de imóveis no distrito. \n",
    "\n",
    "Com todas essas evidências de anomalias em distritos com poucos imóveis, vamos escolher um ponto de corte para eliminar esses distritos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_households(\n",
    "    inches_per_col: float = 6.0,\n",
    "    aspect_ratio: float = 1.5,\n",
    ") -> None:\n",
    "    '''Plots the distribution of the number of households.'''\n",
    "\n",
    "    def _plot_households(\n",
    "        ax: plt.Axes,\n",
    "        feature: pd.Series,\n",
    "        cumulative: bool,\n",
    "        x_label: str | None,\n",
    "        y_label: str | None,\n",
    "        x_fmt: str | None,\n",
    "        y_fmt: str | None,\n",
    "        title: str | None,\n",
    "    ) -> None:\n",
    "        num_items = len(feature)\n",
    "        n_bins = int(np.floor(np.sqrt(num_items)))\n",
    "\n",
    "        ax.hist(feature, bins=n_bins, density=True, cumulative=cumulative)\n",
    "        ax.grid(visible=True, which='both', axis='both')\n",
    "        ax.set_xlabel(x_label)\n",
    "        ax.set_ylabel(y_label)\n",
    "        if x_fmt is not None:\n",
    "            ax.xaxis.set_major_formatter(x_fmt)\n",
    "        if y_fmt is not None:\n",
    "            ax.yaxis.set_major_formatter(y_fmt)\n",
    "        ax.set_title(title)\n",
    "\n",
    "    n_rows = 2\n",
    "    n_cols = 2\n",
    "\n",
    "    width = n_cols * inches_per_col\n",
    "    height = n_rows * inches_per_col / aspect_ratio\n",
    "\n",
    "    feature = X_train_new_features['households']\n",
    "    log_feature = X_train_new_features['households'].map(np.log10)\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=n_rows,\n",
    "        ncols=n_cols,\n",
    "        figsize=(width, height),\n",
    "    )\n",
    "\n",
    "    fig.suptitle('Distribuição de Número de Imóveis por Distrito')\n",
    "\n",
    "    _plot_households(\n",
    "        ax=axes[0, 0],\n",
    "        feature=feature,\n",
    "        cumulative=False,\n",
    "        x_label=None,\n",
    "        y_label='Frequência normalizada',\n",
    "        x_fmt='{x:.0f}',\n",
    "        y_fmt=None,\n",
    "        title='Escala Linear',\n",
    "    )\n",
    "\n",
    "    _plot_households(\n",
    "        ax=axes[0, 1],\n",
    "        feature=log_feature,\n",
    "        cumulative=False,\n",
    "        x_label=None,\n",
    "        y_label=None,\n",
    "        x_fmt='{x:.2f}',\n",
    "        y_fmt='{x:.2f}',\n",
    "        title='Escala Logarítmica',\n",
    "    )\n",
    "\n",
    "    _plot_households(\n",
    "        ax=axes[1, 0],\n",
    "        feature=feature,\n",
    "        cumulative=True,\n",
    "        x_label='Número de Imóveis',\n",
    "        y_label='Frequência normalizada cumulativa',\n",
    "        x_fmt='{x:.0f}',\n",
    "        y_fmt='{x:.2f}',\n",
    "        title=None,\n",
    "    )\n",
    "\n",
    "    _plot_households(\n",
    "        ax=axes[1, 1],\n",
    "        feature=log_feature,\n",
    "        cumulative=True,\n",
    "        x_label=r'$\\log_{10}(\\text{Número de Imóveis})$',\n",
    "        y_label=None,\n",
    "        x_fmt='{x:.2f}',\n",
    "        y_fmt='{x:.2f}',\n",
    "        title=None,\n",
    "    )\n",
    "\n",
    "    fig.tight_layout(w_pad=4.0)\n",
    "\n",
    "\n",
    "plot_households()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que, mais uma vez, a escala logarítmica é mais adequada para estes dados. Nesta escala, o ponto de corte $2.0$ parece bom no \"olhômetro\", vamos ver a qual percentil este ponto corresponde:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_cut_point = 2.0\n",
    "\n",
    "cut_point = 10**log_cut_point\n",
    "\n",
    "values = X_train_new_features['households'].map(np.log10).values\n",
    "quantile = (values < log_cut_point).mean().item()\n",
    "\n",
    "print(f'quantile of {cut_point=:.2f} is {quantile=:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao cortar no valor $2.0$ na escala logarítmica, que corresponde a $100$ imóveis no distrito, estamos ignorando apenas $3.29\\%$ dos dados. Isso parece ser uma perda aceitável mas, novamente, quem tem a última palavra é o *business expert*, ok?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_rows = (X_train_new_features['households'] > cut_point)\n",
    "X_train_new_features = X_train_new_features[good_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, vamos analisar as demais colunas sobre o ponto de vista logaritmico, pois:\n",
    "\n",
    "- São colunas de valor estritamente positivo;\n",
    "- Possuem cauda longa à direita.\n",
    "\n",
    "São situações que sugerem o uso da função $\\log$ para \"gaussianizar\" os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_units = {\n",
    "    'households': 'imóveis',\n",
    "    'median_income': r'USD $\\times 10000$',\n",
    "    'rooms_per_household': 'cômodos/imóvel',\n",
    "    'population_per_household': 'pessoas/imóvel',\n",
    "    'bedrooms_per_room': 'quartos/cômodo',\n",
    "    'median_house_value': r'USD',\n",
    "}\n",
    "\n",
    "columns = list(measurement_units.keys())\n",
    "\n",
    "data_train_new_features = \\\n",
    "    pd.concat([X_train_new_features, y_train], axis=1)\n",
    "\n",
    "data_train_new_features = data_train_new_features \\\n",
    "    .loc[:, columns] \\\n",
    "    .copy() \\\n",
    "    .map(np.log10)\n",
    "\n",
    "num_items = data_train_new_features.shape[0]\n",
    "n_bins = int(np.floor(np.sqrt(num_items)))\n",
    "\n",
    "inches_per_column = 5\n",
    "aspect_ratio = 1.5\n",
    "\n",
    "n_cols = 2\n",
    "n_rows = 3\n",
    "\n",
    "width = n_cols * inches_per_column\n",
    "height = n_rows * inches_per_column / aspect_ratio\n",
    "\n",
    "fig, axes = plt.subplots(ncols=n_cols, nrows=n_rows, figsize=(width, height))\n",
    "fig.suptitle('Distribuição de Variáveis Transformadas')\n",
    "\n",
    "for ax, column in zip(axes.flatten(), columns):\n",
    "    ax.hist(data_train_new_features[column], bins=n_bins, density=True)\n",
    "    ax.set_xlabel(measurement_units[column])\n",
    "    ax.set_ylabel('Frequência normalizada')\n",
    "    ax.set_title(column)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece ser uma boa idéia sim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlações feature-target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_units = {\n",
    "    'longitude': 'graus',\n",
    "    'latitude': 'graus',\n",
    "    'housing_median_age': 'anos',\n",
    "    'households': 'imóveis',\n",
    "    'median_income': r'USD $\\times 10000$',\n",
    "    'rooms_per_household': 'cômodos/imóvel',\n",
    "    'population_per_household': 'pessoas/imóvel',\n",
    "    'bedrooms_per_room': 'quartos/cômodo',\n",
    "    'median_house_value': r'USD',\n",
    "    'ocean_proximity': 'classe',\n",
    "}\n",
    "\n",
    "num_features = [\n",
    "    'longitude',\n",
    "    'latitude',\n",
    "    'housing_median_age',\n",
    "    'households',\n",
    "    'median_income',\n",
    "    'rooms_per_household',\n",
    "    'population_per_household',\n",
    "    'bedrooms_per_room',\n",
    "]\n",
    "\n",
    "cat_features = [\n",
    "    'ocean_proximity',\n",
    "]\n",
    "\n",
    "scale_features = [\n",
    "    'households',\n",
    "    'median_income',\n",
    "    'rooms_per_household',\n",
    "    'population_per_household',\n",
    "    'bedrooms_per_room',\n",
    "]\n",
    "\n",
    "target = 'median_house_value'\n",
    "\n",
    "data_train_new_features = \\\n",
    "    pd.concat([X_train_new_features, y_train], axis=1) \\\n",
    "    .copy()\n",
    "\n",
    "data_train_new_features[scale_features] = \\\n",
    "    data_train_new_features[scale_features].map(np.log10)\n",
    "data_train_new_features[target] = \\\n",
    "    data_train_new_features[target].map(np.log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = data_train_new_features \\\n",
    "    .select_dtypes(include='number') \\\n",
    "    .corr(method='spearman')\n",
    "\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,\n",
    "    cmap='coolwarm',\n",
    "    fmt='.2f',\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inches_per_column = 4\n",
    "aspect_ratio = 1.2\n",
    "\n",
    "n_rows = 3\n",
    "n_cols = 3\n",
    "\n",
    "width = n_cols * inches_per_column\n",
    "height = n_rows * inches_per_column / aspect_ratio\n",
    "\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(width, height))\n",
    "fig.suptitle(\n",
    "    'Relação entre Variáveis Transformadas e o Preço Mediano das Casas')\n",
    "\n",
    "for idx, (ax, feature) in enumerate(zip(axes.flatten(), num_features)):\n",
    "    data_train_new_features.plot.scatter(\n",
    "        x=feature,\n",
    "        y=target,\n",
    "        ax=ax,\n",
    "        alpha=0.05,\n",
    "    )\n",
    "    if idx % n_cols == 0:\n",
    "        y_label = measurement_units[target]\n",
    "    else:\n",
    "        y_label = None\n",
    "\n",
    "    ax.set_title(feature)\n",
    "    ax.set_xlabel(measurement_units[feature])\n",
    "    ax.set_ylabel(y_label)\n",
    "\n",
    "ax = axes.flatten()[-1]\n",
    "data_train_new_features[['ocean_proximity', target]] \\\n",
    "    .plot \\\n",
    "    .box(by='ocean_proximity', ax=ax)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    data_train_new_features['longitude'],\n",
    "    data_train_new_features['latitude'],\n",
    "    c=data_train_new_features['median_house_value'],\n",
    "    cmap='viridis',\n",
    "    s=data_train_new_features['households'] / 10,\n",
    "    alpha=0.3,\n",
    ")\n",
    "plt.colorbar(label=measurement_units[target])\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Preço Mediano das Casas por Localização')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando o número de *features* é tão pequeno, podemos fazer uma análise de correlação mais detalhada. Neste caso, é importante tentar explicar o fenômeno observado com suas palavras, para ajudar no entendimento do *dataset*.\n",
    "\n",
    "Algumas features apresentam forte correlação entre si:\n",
    "\n",
    "- `rooms_per_household` versus `bedrooms_per_room`: alta correlação negativa. Parece legítimo: quanto menos cômodos (um imóvel pequeno), maior o número de quartos em relação a esse pequeno número de cômodos.\n",
    "\n",
    "- `median_income` versus `bedrooms_per_room`: alta correlação negativa. Isso parece vir de modo indireto do fenômeno visto acima, pois quanto menor a renda, menor o imóvel, e (pela observação acima) consequentemente maior o número de quartos em relação ao número de cômodos.\n",
    "\n",
    "- `median_income` versus `rooms_per_household`: moderada correlação positiva. Possivelmente vem do fato de que apenas pessoas de maior poder aquisitivo podem adquirir imóveis com grande número de cômodos. Mas esta correlação não é perfeita pois é possível ter pessoas de alto poder aquisitivo (em termos absolutos) adquirindo imóveis pequenos, quando estes se localizam em uma área muito valorizada.\n",
    "\n",
    "Entre o target `median_house_value` e as features, ressalta-se que:\n",
    "\n",
    "- `median_income` versus `median_house_value`: moderada correlação positiva - maior poder aquisitivo frequentemente (mas nem sempre, ver ressalva acima) resulta em aquisição de imóveis de maior valor.\n",
    "\n",
    "- `house_median_age` versus `median_house_value`: surpreendentemente a correlação é praticamente nula, indicando que a idade mediana dos imóveis no bairro não impacta o valor mediano dos imóveis. Parece que temos de tudo: imóveis novos e caros, novos e baratos, antigos e caros (bairros nobres mais antigos), antigos e baratos.\n",
    "\n",
    "Note que usamos a correlação de Spearman. Procure saber mais sobre esta correlação, e como ela se compara com a correlação usual, chamada correlação linear ou correlação de Pearson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retomando a idéia da filtragem\n",
    "\n",
    "Agora que fizemos toda a análise exploratória que queríamos fazer, vamos consolidar o conhecimento adquirido na forma de funções que realizam:\n",
    "\n",
    "- Leitura dos dados brutos;\n",
    "- Separação treino-teste;\n",
    "- Pré-processamento **não-treinável**. Ou seja: transformações estáticas dos dados, que não requeiram treinamento. Isso se diferencia de pré-processamento **treinável**, que deverá fazer parte da pipeline **treinável** de pré-processamento e de modelagem.\n",
    "\n",
    "Vamos escrever essas funções então. Para testar essas funções, vamos apagar tudo que já rodou neste notebook até aqui tambem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As funções de leitura já estão feitas, vamos somente copiá-las aqui:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "from pathlib import Path\n",
    "from urllib import request\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_housing_data(data_dir: Path) -> None:\n",
    "    '''Downloads the California Housing Prices dataset.\n",
    "\n",
    "    Downloads the California Housing Prices dataset from Aurelien Geron's\n",
    "    GitHub repository and saves it to the specified directory.\n",
    "\n",
    "    Args:\n",
    "        data_dir: The directory to which the dataset will be saved.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    if not data_dir.exists():\n",
    "        data_dir.mkdir(parents=True)\n",
    "\n",
    "    # Fetch the housing data.\n",
    "    HOUSING_URL = ('https://raw.githubusercontent.com/ageron/handson-ml2/'\n",
    "                   'master/datasets/housing/housing.tgz')\n",
    "    tgz_path = data_dir / 'housing.tgz'\n",
    "    request.urlretrieve(HOUSING_URL, tgz_path)\n",
    "\n",
    "    # Extract the housing data.\n",
    "    with tarfile.open(tgz_path) as housing_tgz:\n",
    "        housing_tgz.extractall(path=data_dir, filter='data')\n",
    "\n",
    "\n",
    "def load_housing_data(data_dir: Path) -> pd.DataFrame:\n",
    "    '''Loads the California Housing Prices dataset.\n",
    "\n",
    "    Loads the California Housing Prices dataset from the specified directory.\n",
    "\n",
    "    Args:\n",
    "        data_dir: The directory from which the dataset will be loaded.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the California Housing Prices dataset.\n",
    "    '''\n",
    "    csv_path = data_dir / 'housing.csv'\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As funções de filtragem devem ser atualizadas com o que aprendemos na análise exploratória pós-separação treino-teste. Vamos incorporar a filtragem com o pré-processamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''Pre-processes the California Housing Prices dataset.\n",
    "\n",
    "    Pre-processes the California Housing Prices dataset by removing duplicates\n",
    "    and filtering out invalid rows.\n",
    "\n",
    "    Args:\n",
    "        data: A pandas DataFrame containing the California Housing Prices dataset.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the pre-processed California Housing Prices dataset.\n",
    "    '''\n",
    "    # Remove duplicates.\n",
    "    data = data.drop_duplicates()\n",
    "\n",
    "    # Remove rows with spikes.\n",
    "    valid_rows = ((data['median_income'] < 15) &\n",
    "                  (data['housing_median_age'] < 52) &\n",
    "                  (data['median_house_value'] < 500001))\n",
    "\n",
    "    # Remove rows with ocean_proximity == 'ISLAND'.\n",
    "    valid_rows &= data['ocean_proximity'] != 'ISLAND'\n",
    "\n",
    "    data = data[valid_rows]\n",
    "\n",
    "    # Compute new features.\n",
    "    data['rooms_per_household'] = data['total_rooms'] / data['households']\n",
    "    data['bedrooms_per_room'] = data['total_bedrooms'] / data['total_rooms']\n",
    "    data['population_per_household'] = data['population'] / data['households']\n",
    "\n",
    "    data = data.drop(columns=['total_rooms', 'total_bedrooms', 'population'])\n",
    "\n",
    "    # Apply log transformation to selected features.\n",
    "    scale_features = [\n",
    "        'households',\n",
    "        'median_income',\n",
    "        'rooms_per_household',\n",
    "        'population_per_household',\n",
    "        'bedrooms_per_room',\n",
    "        'median_house_value',\n",
    "    ]\n",
    "\n",
    "    for feature in scale_features:\n",
    "        data[f'log_{feature}'] = data[feature].map(np.log10)\n",
    "\n",
    "    data = data.drop(columns=scale_features)\n",
    "\n",
    "    # Crop outliers.\n",
    "    log_cut_point = 2.0\n",
    "    valid_rows = data['log_households'] > log_cut_point\n",
    "    data = data[valid_rows]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos testar as funções:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path.cwd().parents[2] / 'datasets' / 'housing'\n",
    "\n",
    "print(f'Deleting all files in {DATA_DIR}...')\n",
    "for file in DATA_DIR.glob('*'):\n",
    "    file.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Saving data to {DATA_DIR}')\n",
    "fetch_housing_data(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_housing_data(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = preprocess_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece ok.\n",
    "\n",
    "Idealmente, devemos fazer o seguinte:\n",
    "\n",
    "- Código na forma de funções deverá ir para *módulos* Python (arquivos `.py`). \n",
    "    - Conforme o projeto cresce, é melhor criar múltiplos módulos e organizá-los em um *pacote* Python (um diretório cheio de módulos, sub-pacotes, e obrigatoriamente um arquivo nomeado `__init__.py`). Este pacote deve estar dentro de um diretório `src`, subdiretório com nome do pacote.\n",
    "    - Para ter acesso ao pacote a partir de outros arquivos Python (notebooks, scripts, etc) é importante ter um arquivo `pyproject.toml` descrevendo características do projeto Python em desenvolvimento.\n",
    "- Código exploratório na forma de `notebooks` Python vão em um diretório `notebooks`.\n",
    "- Scripts Python (arquivos `.py` que são executados diretamente) vão em um diretório `scripts`.\n",
    "- Testes (e.g. testes unitários) do pacote Python vão para um diretório `tests`.\n",
    "\n",
    "Vamos então criar essa estrutura na nossa próxima tarefa! Até breve!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
